from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import json
import os
from dotenv import load_dotenv
import google.generativeai as genai
from prompts import PROMPT_FINAL, PROMPT_RESPOND

# Load environment variables åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

app = Flask(__name__)
CORS(app)  # Allow cross-origin requests å…è®¸è·¨åŸŸè¯·æ±‚

# Configure Google Gemini é…ç½® Google Gemini
genai.configure(api_key=GOOGLE_API_KEY)

# ==================== Routes è·¯ç”± ====================

# Serve static files æä¾›é™æ€æ–‡ä»¶æœåŠ¡
@app.route('/')
def index():
    return send_from_directory('.', 'index.html')

@app.route('/<path:filename>')
def static_files(filename):
    return send_from_directory('.', filename)

# Unified AI analysis endpoint ç»Ÿä¸€çš„AIåˆ†ææ¥å£
@app.route('/api/analyze', methods=['POST'])
def analyze_content():
    try:
        # Get request data è·å–è¯·æ±‚æ•°æ®
        data = request.get_json()
        content = data.get('content', '').strip()
        
        if not content:
            return jsonify({'error': 'å†…å®¹ä¸èƒ½ä¸ºç©º'}), 400
        
        # Call unified analysis function è°ƒç”¨ç»Ÿä¸€çš„åˆ†æå‡½æ•°
        analysis = analyze_with_ai(content)
        
        return jsonify({
            'success': True,
            'comments': analysis
        })
        
    except Exception as e:
        import traceback
        print(f'===== AI Analysis Error AIåˆ†æé”™è¯¯ =====')
        print(f'Error Type é”™è¯¯ç±»å‹: {type(e).__name__}')
        print(f'Error Message é”™è¯¯ä¿¡æ¯: {str(e)}')
        print(f'Full Stack å®Œæ•´å †æ ˆ:')
        traceback.print_exc()
        
        # If exception object contains AI raw response, print it å¦‚æœå¼‚å¸¸å¯¹è±¡åŒ…å«AIåŸå§‹å“åº”ï¼Œæ‰“å°å®ƒ
        if hasattr(e, 'ai_response'):
            print(f'===== AI Raw Output AIåŸå§‹è¾“å‡º =====')
            print(e.ai_response)
        
        print(f'=====================')
        return jsonify({
            'error': 'AIåˆ†æå¤±è´¥',
            'message': str(e)
        }), 500

# AI response endpoint AIå›åº”æ¥å£
@app.route('/api/respond', methods=['POST'])
def respond_to_question():
    try:
        # Get request data è·å–è¯·æ±‚æ•°æ®
        data = request.get_json()
        comment_id = data.get('commentId')
        response = data.get('response', '').strip()
        original_question = data.get('originalQuestion', '')  # Get original question è·å–åŸå§‹é—®é¢˜
        conversation_history = data.get('conversationHistory', [])  # Get conversation history è·å–å¯¹è¯å†å²
        
        if not response:
            return jsonify({'error': 'å›ç­”å†…å®¹ä¸èƒ½ä¸ºç©º'}), 400
        
        # Call AI response function, pass in original question, user answer, and conversation history è°ƒç”¨AIå›åº”å‡½æ•°ï¼Œä¼ å…¥åŸå§‹é—®é¢˜ã€ç”¨æˆ·å›ç­”å’Œå¯¹è¯å†å²
        feedback_data = respond_with_ai(response, original_question, conversation_history)
        
        return jsonify({
            'success': True,
            'understood': feedback_data.get('understood', True),
            'feedback': feedback_data.get('feedback', ''),
            'followUpQuestion': feedback_data.get('followUpQuestion', None)
        })
        
    except Exception as e:
        import traceback
        print(f'===== AI Response Error AIå›åº”é”™è¯¯ =====')
        print(f'Error Type é”™è¯¯ç±»å‹: {type(e).__name__}')
        print(f'Error Message é”™è¯¯ä¿¡æ¯: {str(e)}')
        print(f'Full Stack å®Œæ•´å †æ ˆ:')
        traceback.print_exc()
        
        # If exception object contains AI raw response, print it å¦‚æœå¼‚å¸¸å¯¹è±¡åŒ…å«AIåŸå§‹å“åº”ï¼Œæ‰“å°å®ƒ
        if hasattr(e, 'ai_response'):
            print(f'===== AI Raw Output AIåŸå§‹è¾“å‡º =====')
            print(e.ai_response)
        
        print(f'=====================')
        return jsonify({
            'error': 'AIå›åº”å¤±è´¥',
            'message': str(e)
        }), 500

# ==================== AI Functions AI å‡½æ•° ====================

def clean_json_response(text):
    """
    Clean AI response, remove markdown code block markers
    æ¸…ç†AIå“åº”ï¼Œç§»é™¤markdownä»£ç å—æ ‡è®°
    
    Args:
        text: Raw text returned by AI AIè¿”å›çš„åŸå§‹æ–‡æœ¬
    
    Returns:
        str: Cleaned JSON string æ¸…ç†åçš„JSONå­—ç¬¦ä¸²
    """
    text = text.strip()
    
    # Remove markdown code block markers ç§»é™¤markdownä»£ç å—æ ‡è®°
    if text.startswith('```json'):
        text = text[7:]  # Remove opening ```json ç§»é™¤å¼€å¤´çš„ ```json
    elif text.startswith('```'):
        text = text[3:]  # Remove opening ``` ç§»é™¤å¼€å¤´çš„ ```
    
    if text.endswith('```'):
        text = text[:-3]  # Remove closing ``` ç§»é™¤ç»“å°¾çš„ ```
    
    # Clean all leading and trailing whitespace again (including newlines) å†æ¬¡æ¸…ç†æ‰€æœ‰å‰åç©ºç™½å­—ç¬¦ï¼ˆåŒ…æ‹¬æ¢è¡Œç¬¦ï¼‰
    text = text.strip()
    
    # Find position of first '[' or '{' (JSON start) æ‰¾åˆ°ç¬¬ä¸€ä¸ª '[' æˆ– '{' çš„ä½ç½®ï¼ˆJSONçš„å¼€å§‹ï¼‰
    json_start = -1
    for i, char in enumerate(text):
        if char in '[{':
            json_start = i
            break
    
    if json_start > 0:
        text = text[json_start:]
    
    # Find position of last ']' or '}' (JSON end) æ‰¾åˆ°æœ€åä¸€ä¸ª ']' æˆ– '}' çš„ä½ç½®ï¼ˆJSONçš„ç»“æŸï¼‰
    json_end = -1
    for i in range(len(text) - 1, -1, -1):
        if text[i] in ']}':
            json_end = i + 1
            break
    
    if json_end > 0:
        text = text[:json_end]
    
    return text.strip()

def print_ai_response(response_text, response_type='analysis'):
    """
    Print AI response content
    æ‰“å°AIå“åº”å†…å®¹
    
    Args:
        response_text: Raw text of AI response AIå“åº”çš„åŸå§‹æ–‡æœ¬
        response_type: Response type ('analysis' or 'feedback') å“åº”ç±»å‹ ('analysis' æˆ– 'feedback')
    """
    print(f'\n{"="*60}')
    print(f'AIå“åº” [{response_type.upper()}] - {__import__("datetime").datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
    print(f'{"="*60}')
    print(response_text)
    print(f'{"="*60}\n')

def analyze_with_ai(content):
    """
    Unified AI analysis function
    ç»Ÿä¸€çš„ AI åˆ†æå‡½æ•°
    
    Args:
        content: User's explanation content ç”¨æˆ·è®²è§£çš„å†…å®¹
    
    Returns:
        list: List of AI-generated comments AI ç”Ÿæˆçš„è¯„è®ºåˆ—è¡¨
    """
    ai_response = None  # For error handling access ç”¨äºé”™è¯¯å¤„ç†æ—¶è®¿é—®
    
    try:
        # Use PROMPT_FINAL template ä½¿ç”¨ PROMPT_FINAL æ¨¡æ¿
        prompt = PROMPT_FINAL.format(content=content)
        
        # Initialize Gemini model åˆå§‹åŒ– Gemini æ¨¡å‹
        model = genai.GenerativeModel('gemini-2.0-flash')
        
        # Generate response ç”Ÿæˆå›å¤
        response = model.generate_content(prompt)
        ai_response = response.text
        
        # Print AI raw response æ‰“å°AIåŸå§‹å“åº”
        print_ai_response(ai_response, 'analysis')
        
        # Clean response text, remove markdown code block markers æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤markdownä»£ç å—æ ‡è®°
        cleaned_response = clean_json_response(ai_response)
        
        # Try to parse JSON response å°è¯•è§£æJSONå“åº”
        try:
            return json.loads(cleaned_response)
        except json.JSONDecodeError as e:
            # If AI returns incorrect format, throw error å¦‚æœAIè¿”å›æ ¼å¼ä¸æ­£ç¡®ï¼ŒæŠ›å‡ºé”™è¯¯
            print(f'===== AI Raw Response AIåŸå§‹å“åº” =====')
            print(ai_response)
            print(f'===== Cleaned Response æ¸…ç†åçš„å“åº” =====')
            print(cleaned_response)
            print(f'===== JSON Parse Error JSONè§£æé”™è¯¯ =====')
            print(f'Error Position é”™è¯¯ä½ç½®: {e.pos}, Line è¡Œ: {e.lineno}, Column åˆ—: {e.colno}')
            print(f'Error Message é”™è¯¯ä¿¡æ¯: {e.msg}')
            error = ValueError(f'AIè¿”å›çš„å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {str(e)}')
            error.ai_response = ai_response  # Attach AI response é™„åŠ AIå“åº”
            raise error
            
    except Exception as e:
        print(f'Google Gemini APIè°ƒç”¨å¤±è´¥: {e}')
        if ai_response and not hasattr(e, 'ai_response'):
            e.ai_response = ai_response  # Attach AI response to exception é™„åŠ AIå“åº”åˆ°å¼‚å¸¸
        raise

def respond_with_ai(user_response, original_question='', conversation_history=None):
    """
    Use Google Gemini to provide feedback on user's answer
    ä½¿ç”¨ Google Gemini å¯¹ç”¨æˆ·çš„å›ç­”è¿›è¡Œåé¦ˆ
    
    Args:
        user_response: User's answer content ç”¨æˆ·çš„å›ç­”å†…å®¹
        original_question: Original question previously asked by AI AIä¹‹å‰æå‡ºçš„åŸå§‹é—®é¢˜
        conversation_history: List of previous Q&A exchanges ä¹‹å‰çš„é—®ç­”äº¤æµåˆ—è¡¨
    
    Returns:
        dict: AI feedback object, containing understood, feedback, followUpQuestion
              AI çš„åé¦ˆå¯¹è±¡ï¼ŒåŒ…å« understood, feedback, followUpQuestion
    """
    ai_response = None  # For error handling access ç”¨äºé”™è¯¯å¤„ç†æ—¶è®¿é—®
    
    if conversation_history is None:
        conversation_history = []
    
    # Build conversation context if there's history å¦‚æœæœ‰å†å²è®°å½•ï¼Œæ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
    context = ""
    if conversation_history:
        context = "\n\n## Previous Conversation History ä¹‹å‰çš„å¯¹è¯å†å²:\n"
        for i, exchange in enumerate(conversation_history, 1):
            context += f"\n**Round {i} ç¬¬{i}è½®:**\n"
            context += f"AI Question AIé—®é¢˜: {exchange.get('question', '')}\n"
            context += f"Teacher Answer è€å¸ˆå›ç­”: {exchange.get('answer', '')}\n"
    
    # Build prompt with original question, user answer, and conversation history ä½¿ç”¨åŸå§‹é—®é¢˜ã€ç”¨æˆ·å›ç­”å’Œå¯¹è¯å†å²æ„å»ºæç¤ºè¯
    # If no original question, provide a more reasonable default value å¦‚æœæ²¡æœ‰åŸå§‹é—®é¢˜ï¼Œæä¾›ä¸€ä¸ªæ›´åˆç†çš„é»˜è®¤å€¼
    prompt = PROMPT_RESPOND.format(
        previous_question=original_question if original_question else "ä¹‹å‰è®¨è®ºçš„æ¦‚å¿µæˆ–é—®é¢˜",
        teacher_answer=user_response
    )
    
    # Append conversation context to prompt å°†å¯¹è¯ä¸Šä¸‹æ–‡é™„åŠ åˆ°æç¤ºè¯
    if context:
        prompt = prompt + context
    
    try:
        # Initialize Gemini model åˆå§‹åŒ– Gemini æ¨¡å‹
        model = genai.GenerativeModel('gemini-2.0-flash')
        
        # Generate response ç”Ÿæˆå›å¤
        response = model.generate_content(prompt)
        ai_response = response.text.strip()
        
        # Print AI feedback response æ‰“å°AIåé¦ˆå“åº”
        print_ai_response(ai_response, 'feedback')
        
        # Clean and parse JSON response æ¸…ç†å’Œè§£æJSONå“åº”
        cleaned_response = clean_json_response(ai_response)
        
        try:
            feedback_data = json.loads(cleaned_response)
            
            # Validate required fields éªŒè¯å¿…éœ€å­—æ®µ
            if 'understood' not in feedback_data or 'feedback' not in feedback_data:
                raise ValueError('AIè¿”å›çš„åé¦ˆç¼ºå°‘å¿…éœ€å­—æ®µ')
            
            return feedback_data
            
        except json.JSONDecodeError as e:
            # If JSON parsing fails, return text format fallback å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›æ–‡æœ¬æ ¼å¼çš„å…œåº•æ–¹æ¡ˆ
            print(f'===== AI Feedback JSON Parse Failed AIåé¦ˆJSONè§£æå¤±è´¥ =====')
            print(f'Raw Response åŸå§‹å“åº”: {ai_response}')
            print(f'Cleaned æ¸…ç†å: {cleaned_response}')
            print(f'Error é”™è¯¯: {str(e)}')
            print(f'===== Using Fallback ä½¿ç”¨å…œåº•æ–¹æ¡ˆ =====')
            
            # Fallback: assume fully understood, return text content å…œåº•æ–¹æ¡ˆï¼šå‡è®¾å®Œå…¨ç†è§£ï¼Œè¿”å›æ–‡æœ¬å†…å®¹
            return {
                'understood': True,
                'feedback': ai_response,
                'followUpQuestion': None
            }
            
    except Exception as e:
        print(f'Google Gemini APIè°ƒç”¨å¤±è´¥: {e}')
        if ai_response and not hasattr(e, 'ai_response'):
            e.ai_response = ai_response  # Attach AI response to exception é™„åŠ AIå“åº”åˆ°å¼‚å¸¸
        raise

# ==================== Start Service å¯åŠ¨æœåŠ¡ ====================

if __name__ == '__main__':
    port = int(os.getenv('PORT', 10001))
    debug_mode = os.getenv('FLASK_ENV') == 'development'
    
    print(f'ğŸš€ è´¹æ›¼å­¦ä¹ åŠ©æ‰‹è¿è¡Œåœ¨ http://localhost:{port}')
    app.run(host='127.0.0.1', port=port, debug=debug_mode)